---
title: "Portfolio for Computational Musicology"
author: "Nora Saleh"
output:
  flexdashboard::flex_dashboard:
    self_contained: false
    orientation: columns
    vertical_layout: scroll
    theme:
      version: 4
      bootswatch: minty
---

```{=html}
<style>
.dataTables_scrollBody {
    height:540px !important;
}
.chart-stage-flex {
    overflow:auto !important;
}
</style>
```

```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(ggplot2)
library(gapminder)
library(tidyverse)
library(usethis)
library(plotly)
library(remotes)
library(compmus)

library(tidymodels)
library(heatmaply)
library(protoclust)

secondgg <- readRDS(file = "data/secondgg-data.RDS")
fourthgg <- readRDS(file = "data/fourthgg-data.RDS")
secondbg <- readRDS(file = "data/secondbg-data.RDS")
fourthbg <- readRDS(file = "data/fourthbg-data.RDS")

corpus <- readRDS(file = "data/corpus-data.RDS")
```
-----------------------------------------------------------------------
# Introduction

## Column

### Corpus Description {style="min-height:400px"}
The corpus will consist of K-pop songs spanning two distinct eras in the genre's evolution: the second generation (2005-2011) and the fourth generation (2018-2022). Note that the term "generation" is used as a whole movement within the industry: a change in fan culture, trends, marketing tactics and ofcourse the music. K-pop is getting increasingly more popular outside South-Korea, captivating audiences with its infectious melodies. Through this corpus, I aim to explore the diverse sounds, themes, and trends within the genre, shedding light on its evolution. The second generation witnessed the rise of iconic groups like Girls' Generation and Big Bang. Some of the most popular ones include “Gee” (Girls Generation), “Ring Ding Dong” (SHINee), “Sorry Sorry” (Super Junior), “I Am The Best” (2NE1), and “FANTASTIC BABY” (BIGBANG). Girls sported outfits that were poppy and colorful, whereas, guys were seen in suits, tees, jeans. In contrast, the 4th generation marked a new era of global influence. The most popular groups today include Stray Kids, ATEEZ, (G)I-DLE, TXT, ITZY, ENHYPEN, aespa, IVE, LE SSERAFIM, and NewJeans. What took years to achieve success in the yesteryears, many fourth generation groups were capable of achieving in a matter of a few months. Noticeably, there has been an increased focus on creating easy hooks for building viral social media challenges. Popular songs include “LATATA” ((G)I-DLE), “WONDERLAND” (ATEEZ), “WANNABE” (ITZY), “God’s Menu” (Stray Kids), “Savage” (aespa). I expect significant differences between the 2nd and 4th generation tracks in terms of sound, production techniques. The 4th generation is likely to showcase a broader range of genres, experimental production styles. I chose the corpus based on how many Spotify streams the songs have.


***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0XlcubxUyMwSOP3GOgzbQR?utm_source=generator" width="60%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/47AEDX1Vkw4Nh0ipx8eWTp?utm_source=generator" width="60%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0UqgifAkmHt86pzVTfBcmS?utm_source=generator&theme=0" width="60%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/53gZC84IKZk5QoOqSwgUg9?utm_source=generator" width="60%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


# Global Analysis {data-navmenu="Analysis" data-orientation=rows}

## Row {style="min-height:600px"}

### Description {data-width="350"}

#### Analysis over the entire range of songs

The multivariate analysis plot shows a comparison of the liveness, acousticness and the speechiness of each individual song in the corpus. There appears to be very a large bunch of songs with low acousticness and liveness. The speechiness on the other hand seems all over the place, likely due to the vast differences in songs


### Multivariate Analysis {data-width="650"}

```{r, echo=FALSE}
corpus_with_title <-
  corpus %>%
  mutate(title = paste(track.album.name, " - ", track.name))


multi_plot_1 <- ggplot(corpus_with_title, aes(x = liveness, y = acousticness, color = speechiness, size = speechiness, label = title)) +
  geom_jitter(alpha = 0.7, width = 0.1) +
  theme_classic() +
  scale_color_viridis_c() +
  guides(size = "none") +
  labs(title = "Comparison of Liveness, Acousticness and Speechness",
       subtitle = "The plot consists of the liveness, acousticness and speechness of all individual songs"
  ) +
  theme(
    legend.position = "Bottom"
  )

ggplotly(multi_plot_1, tooltip = c("x", "y", "label", "size"))
```

## Row {style="min-height:580px"}

### Keys and Tempo's

We see in the two right plots a distribution of the key mode and tempo over all songs. The tempo distribution shows that the songs in the corpus is highly densed between 97 and 135 BPM with a big peak at around 130 BPM and a smaller peak at around 113 BPM. The mean tempo of the corpus is 123 BPM which seems to be a little higher than most pop songs. According to the key mode plot, B minor is the most common with 15 occurrences in the corpus followed closely by F# minor with 14 occurrences. What is interesting, is that all keys that Spotify assigns are represented at least once.

### Density Distribution of Tempo

```{r, echo=FALSE}

keysPlot <- ggplot(corpus, aes(key_mode)) +
  stat_count(fill = "pink2", color="pink4", alpha=.9, width=0.5, position = "dodge") +
  labs(x = "Key",
       y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10))

mean_tempo <- mean(corpus$tempo)

tempoPlot <- corpus %>% filter(tempo > 0) %>% ggplot(aes(tempo)) +
  geom_histogram(aes(y=..density..), bins=30, fill="aquamarine3", color="aquamarine4") +
  geom_density() +
  labs(x = "Tempo in BPM",
       y = "Density") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10)) +
  geom_vline(xintercept = mean_tempo, color = "brown1", linetype = 2) +
  annotate(
    "text",
    x = mean_tempo + 6, y = 0.017,
    label = "Mean\nTempo",
    vjust = 1, size = 3, color = "gray4"
  )

ggplotly(tempoPlot)

```

### Frequency of Key's

```{r, echo=FALSE}
ggplotly(keysPlot)

```



# Track-Level Summaries {data-navmenu="Analysis" data-orientation=rows}

## Row {style="min-height:300px"}

### Description

We can see in the plot the tempo and it's standard deviation, volume and song duration. I have made a separation between music released by girl groups and boy groups since the styles differ and to hopefully have more interesting findings. We can see a similar distribution in tempo as was seen before. All duration are also similar and resemble typical song lengths. The only notable difference is that boy groups seem to have a broader range of duration distribution.

## Row {style="min-height:600px"}

### Tempo with Duration & Volume
```{r, echo=FALSE}
bebop <-
  get_playlist_audio_features(
    "",
    "53gZC84IKZk5QoOqSwgUg9"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
bigband <-
  get_playlist_audio_features(
    "",
    "0XlcubxUyMwSOP3GOgzbQR"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
jazz <-
  bebop |>
  mutate(genre = "2nd gen girl group") |>
  bind_rows(bigband |> mutate(genre = "4th gen girl group"))

jazz |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = genre,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )

```

### Tempo with Duration & Volume
```{r, echo=FALSE}
secondb <-
  get_playlist_audio_features(
    "",
    "0UqgifAkmHt86pzVTfBcmS"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
fourthb <-
  get_playlist_audio_features(
    "",
    "47AEDX1Vkw4Nh0ipx8eWTp"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

boy <-
  secondb |>
  mutate(genre = "2nd gen boy group") |>
  bind_rows(fourthb |> mutate(genre = "4th gen boy group"))

boy |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = genre,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )
```

## Row {style="min-height:800px"}

### Other Features
```{r, echo=FALSE}
tempogram <- readRDS(file = "data/tempogram-plot.RDS")
tempogram

```

# Key and Chord Estimation {data-navmenu="Analysis" data-orientation=rows}

## Row {style="min-height:600px"}

### Description

The song is mostly in the key of A and a minor mode. There are short breaks in between, this is when there is no singing.

### Key and Chord Estimation
```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("1p9damTV6h7u7THZKZB2tW") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "T-ARA - Roly Poly", x = "Time (s)", y = "")

```


# Conclusion
This research utilized various techniques such as comparative plot analysis and clustering. Through the course of this analysis it has been become clear that within the genre of k-pop and a specific time frame, the music varies. 
Separating by girl and boy groups, but also the genres within the k-pop genre would help with having more interesting findings. 

